# ğŸš† Train Express Tracker

**Train Express Tracker** is a containerized data engineering & visualization project that tracks real-time train movements in India using the **Indian Railways API**, processes and transforms the data with **PySpark**, stores it in **PostgreSQL**, and visualizes it using **Apache Superset** â€” all orchestrated via **Docker Compose**.

---

## ğŸ“œ Project Overview

This project automates the collection, transformation, and visualization of live train running status data:

1. **Data Ingestion** â€“ Fetches live train data from the **Indian Railways API** inside a Dockerized service.
2. **Data Preprocessing & Transformation** â€“ Uses **PySpark** for:
   - Cleaning & normalizing raw API data.
   - Handling missing values & inconsistent formats.
   - Creating analytics-ready datasets.
3. **Data Storage** â€“ Stores processed data in a **PostgreSQL** database running in a Docker container.
4. **Visualization & Insights** â€“ Connects **Apache Superset** (also in Docker) to PostgreSQL for:
   - Real-time dashboards.
   - Delay and route performance analysis.
   - Interactive geo-visualizations.

---

## ğŸ— Architecture

![Architecture Diagram](Indian-Express-Train-Tracker.png)


### Key Components:

- Indian Railways API â€“ Live train data source.

- PySpark (Docker) â€“ ETL processing and transformations.

- PostgreSQL (Docker) â€“ Data warehouse.

- Apache Superset (Docker) â€“ Visualization platform.

- Docker Compose â€“ Service orchestration.

## âš™ï¸ Tech Stack
| Component        | Technology Used        |
| ---------------- | ---------------------- |
| Language         | Python                 |
| Data Processing  | PySpark                |
| Database         | PostgreSQL             |
| Visualization    | Apache Superset        |
| API Source       | Indian Railways API    |
| Containerization | Docker, Docker Compose |

## ğŸ“‚ Folder Structure
```plaintext
indian-train-live-tracker/
â”œâ”€â”€ README.md
â”œâ”€â”€ config
â”‚   â””â”€â”€ superset_config.py
â”œâ”€â”€ data
â”œâ”€â”€ db_backups
â”‚   â”œâ”€â”€ ba
â”‚   â””â”€â”€ backup.sql
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ hadoop
â”‚   â””â”€â”€ dfs
â”‚       â”œâ”€â”€ data
â”‚       â””â”€â”€ name
â”œâ”€â”€ hadoop-config
â”‚   â”œâ”€â”€ core-site.xml
â”‚   â””â”€â”€ hdfs-site.xml
â”œâ”€â”€ indian-railway-tracker
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ alter_tables.py
â”‚       â”œâ”€â”€ create_tables.py
â”‚       â”œâ”€â”€ fetch_schema.py
â”‚       â””â”€â”€ main.py
â”œâ”€â”€ irctc-connect-main
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ controller
â”‚   â”‚   â””â”€â”€ train.controller.js
â”‚   â”œâ”€â”€ data
â”‚   â”œâ”€â”€ index.d.ts
â”‚   â”œâ”€â”€ routes
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ server.js
â”‚   â””â”€â”€ utils
â”‚       â””â”€â”€ utils.js
â”œâ”€â”€ kafka_data
â”‚   â”œâ”€â”€ cleaner-offset-checkpoint
â”‚   â”œâ”€â”€ log-start-offset-checkpoint
â”‚   â”œâ”€â”€ meta.properties
â”‚   â”œâ”€â”€ recovery-point-offset-checkpoint
â”‚   â””â”€â”€ replication-offset-checkpoint
â”œâ”€â”€ notebooks
â”œâ”€â”€ postgres_data
â”œâ”€â”€ read_dir.py
â”œâ”€â”€ utils
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py
```

## ğŸš€ Setup & Installation (Docker)
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/train-express-tracker.git
cd train-express-tracker

```
### 2ï¸âƒ£ Create Environment Variables

Create a .env file:
```bash
RAILWAY_API_KEY=your_api_key_here
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
POSTGRES_DB=train_tracker
POSTGRES_PORT=5432
```
### 3ï¸âƒ£ Start All Services
```bash
docker compose up -d --build
```

This will spin up:
- PySpark (for ETL jobs)
- PostgreSQL (for storage)
- Apache Superset (for dashboards)

### 4ï¸âƒ£ Access Superset
- Open http://localhost:8088
- Connect Superset to the PostgreSQL container.
- Import superset_dashboards/dashboard_config.json for prebuilt dashboards.

## ğŸ›  Future Enhancements

- Add Apache Airflow container for automated scheduling.
- Integrate predictive delay analytics.
- Improve geo-mapping for better route visuals.