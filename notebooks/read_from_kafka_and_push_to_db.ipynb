{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089fac17-4fdd-4019-8362-40ccba7d2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "# from pyspark.sql.functions import col\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType,\n",
    "    TimestampType, ArrayType\n",
    ")\n",
    "# from pyspark.sql.functions import col, to_timestamp, lit, coalesce, from_json, explode\n",
    "from pyspark.sql.functions import col, from_json, explode, max as spark_max, min as spark_min, regexp_replace\n",
    "import psycopg2\n",
    "from pprint import pprint\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e5cc74-b13c-437f-b87e-17b3388ccebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-08-2025 20_08_2025\n"
     ]
    }
   ],
   "source": [
    "date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "date_str_api = date_str\n",
    "date_str_table = date_str.replace('-', '_')\n",
    "print(date_str, date_str_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc0cef0-7835-498c-843e-cfa89fcd7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ReadLiveStatusFromKafka\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.postgresql:postgresql:42.7.4,\"\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294caac8-9c40-4055-adda-7b8fc0a92190",
   "metadata": {},
   "outputs": [],
   "source": [
    "JDBC_URL = \"jdbc:postgresql://postgres:5432/railway\"\n",
    "DB_PROPERTIES = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"iaCkmHPhuyhFLEBDGdwxQGGqlHvdgWJA\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cef2f86-ec7a-410e-afa0-37bbca8b4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\", \"live-train-status\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc6d6ee-00f3-4d0f-9b3b-c6e9a2f9c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df = raw_df.selectExpr(\n",
    "    \"CAST(key AS STRING) as train_number\",\n",
    "    \"CAST(value AS STRING) as stations_json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b15e41-c1be-4f2e-895b-4fc82d839d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"index\",     StringType(), True),\n",
    "        StructField(\"station\",   StringType(), True),\n",
    "        StructField(\"arr\",       StringType(), True),\n",
    "        StructField(\"dep\",       StringType(), True),\n",
    "        StructField(\"delay\",     StringType(), True),\n",
    "        StructField(\"status\",    StringType(), True),\n",
    "        StructField(\"current\",   StringType(), True),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9eb691b-85f8-44e5-a3ac-8fe8f1bca03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = decoded_df.withColumn(\"stations\", from_json(\"stations_json\", station_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42a709e-382f-4db5-9323-4815d482cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df = parsed_df.withColumn(\"stations\", from_json(col(\"stations_json\"), station_schema)) \\\n",
    "                       .withColumn(\"station\", explode(\"stations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c058ed9a-9d9a-477e-a5d9-44bd23de1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = exploded_df.select(\n",
    "    \"train_number\",\n",
    "    col(\"station.station\").alias(\"station\"),\n",
    "    col(\"station.status\").alias(\"status\"),\n",
    "    regexp_replace(col(\"station.delay\"), \"[^0-9]\", \"\").cast(\"bigint\").alias(\"delay_minutes\"),\n",
    "    col(\"station.index\").cast(\"int\").alias(\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282d0a37-b398-4e60-9033-b9eae130fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(batch_df, batch_id):\n",
    "    last_crossed_df = batch_df.filter(\"status = 'crossed'\") \\\n",
    "        .groupBy(\"train_number\").agg(spark_max(\"station\").alias(\"last_crossed\"))\n",
    "\n",
    "    next_upcoming_df = batch_df.filter(\"status = 'upcoming'\") \\\n",
    "        .groupBy(\"train_number\").agg(spark_min(\"station\").alias(\"next_upcoming\"))\n",
    "\n",
    "    delay_df = batch_df.groupBy(\"train_number\").agg(spark_max(\"delay_minutes\").alias(\"delay_minutes\"))\n",
    "\n",
    "    final_df = last_crossed_df \\\n",
    "        .join(next_upcoming_df, \"train_number\", \"left\") \\\n",
    "        .join(delay_df, \"train_number\", \"left\")\n",
    "\n",
    "    final_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/railway\") \\\n",
    "        .option(\"dbtable\", f\"live_status_{date_str_table}\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"iaCkmHPhuyhFLEBDGdwxQGGqlHvdgWJA\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89922d1-ba05-4752-b235-df551f05b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = station_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .option(\"checkpointLocation\", f\"/home/jupyter/work/checkpoint/train_status_{date_str}/\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9107e-e618-4529-8026-a00c9d492232",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04706cbb-e647-4cc6-b15f-ceb86466f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))\n",
    "print(spark.sparkContext._jvm.java.lang.Class.forName(\"org.postgresql.Driver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1582ee-e88f-4e1d-9edc-5b8cd2ea898e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
